{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### SparkSession\n",
    "- SparkSession是Spark应用程序的统一切入点；它是在Spark 2.0中引入的。它充当Spark所有底层功能的连接器，包括RDD、DataFrames和Datasets，提供统一的接口来处理结构化数据。它是您在开发SparkSQL应用程序时创建的第一批对象之一。作为Spark开发人员，您使用SparkSession.builder()方法创建SparkSession.\n",
    "- SparkSession将几个以前独立的上下文（如SQLContext、HiveContext和StreamingContext）整合到一个切入点中，简化了与Spark及其不同API的交互。它使用户能够执行各种操作，如从各种来源读取数据、执行SQL查询、创建数据帧和数据集，以及有效地对分布式数据集执行操作。\n",
    "#### 方法\n",
    "- master()：通过设置可以使spark以不同模式运行，如local、yarn、standalone cluster等。\n",
    "- appName()：设置spark应用的名称，可以在Spark UI中看到。\n",
    "- getOrCreate()：如果存在SparkSession，则返回现有的SparkSession，否则创建一个新的SparkSession。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "271b8c4c593b5df0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fd550851070>\n",
      "spark version: pyspark-example\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"pyspark-example\").getOrCreate()\n",
    "print(spark)\n",
    "print(\"spark version:\",spark.sparkContext.appName)\n",
    "# 停止\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-28T14:32:45.026716Z",
     "start_time": "2025-07-28T14:32:44.270554Z"
    }
   },
   "id": "initial_id",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SparkContext\n",
    "- Pyspark.SparkContext是PySpark功能的切入点，用于与集群通信以及创建RDD、累加器和广播变量。通过示例学习如何创建PySpark.SparkContext。\n",
    "- spark驱动程序创建并使用 SparkContext 连接到集群管理器以提交 PySpark 作业，请注意，每个JVM只能创建一个SparkContext，为了首先创建另一个，您需要使用stop（）方法停止现有的SparkContext。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14015acd45c40809"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[1] appName=pyspark-example>\n",
      "spark app name: pyspark-example\n"
     ]
    }
   ],
   "source": [
    "# pyspark2.0开始， 创建SparkSession 会在内部创建一个 SparkContext 并公开要使用的 parkContext 变量。\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"pyspark-example\").getOrCreate()\n",
    "print(spark.sparkContext)\n",
    "print(\"spark app name:\",spark.sparkContext.appName)\n",
    "# 停止\n",
    "spark.sparkContext.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T14:32:54.687372Z",
     "start_time": "2025-07-28T14:32:53.993912Z"
    }
   },
   "id": "3d6a4269ac30d0b4",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark_Example_App\n",
      "Spark_Example_App\n"
     ]
    }
   ],
   "source": [
    "# SparkContext的创建\n",
    "from pyspark import SparkContext,SparkConf\n",
    "sc = SparkContext(\"local\", \"Spark_Example_App\")\n",
    "print(sc.appName)\n",
    "\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[1]\").setAppName(\"pyspark-example\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc.appName)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T14:46:29.841216Z",
     "start_time": "2025-07-28T14:46:29.542463Z"
    }
   },
   "id": "dc6926b0b3f5e492",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-1753713989747\n"
     ]
    }
   ],
   "source": [
    "print(sc.applicationId)\n",
    "# print(sc.uiWebUrl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T14:58:31.158394Z",
     "start_time": "2025-07-28T14:58:31.137362Z"
    }
   },
   "id": "f7774fc668f9a4d6",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RDD\n",
    "- RDD（弹性分布式数据集）是PySpark的核心构建块，它是一个容错的、不可变的、分布式的对象集合，不可变意味着一旦创建了RDD，就不能更改它。RDD内的数据被分割成逻辑分区，允许跨集群内的多个节点进行分布式计算。\n",
    "- RDD是类似于Python中列表的对象集合；不同之处在于RDD是在分散在多个物理服务器（也称为集群中的节点）上的多个进程上计算的，而Python集合仅在一个进程中存在和处理。\n",
    "- RDD可以并行处理，并且可以在集群中的多个节点上存储和计算。RDD支持两种类型的操作：转换transformation（如map、filter）和行动action（如count、collect）。\n",
    "\n",
    "#### RDD特性\n",
    "- 不可变性：一旦创建，RDD就不能更改。任何操作都会生成一个新的RDD。\n",
    "- 容错性：RDD在计算过程中会自动处理节点故障。它通过记录转换操作的血统信息来实现容错，这样在节点失败时可以重新计算丢失的数据。\n",
    "- 懒惰求值：RDD的操作是懒惰的，只有在需要结果时才会执行。这意味着RDD的转换操作不会立即计算，而是记录下转换的血统信息，直到执行行动操作时才会触发实际计算。\n",
    "- 分区（并行）：RDD的数据被分割成多个分区，这些分区可以在集群中的不同节点上并行处理。每个分区是一个逻辑上的数据块，可以独立地进行计算。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fa267a0fcda491c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD content: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Number of partitions: 3\n",
      "RDD content: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# 创建RDD ：sparkContext.parallelize()\n",
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data)\n",
    "# 查看RDD的分区数\n",
    "print(\"Number of partitions:\", rdd.getNumPartitions())\n",
    "print(\"RDD content:\", rdd.collect())\n",
    "#手动设置分区数\n",
    "rdd1 = sc.parallelize(data, 3)\n",
    "print(\"Number of partitions:\", rdd1.getNumPartitions())\n",
    "print(\"RDD content:\", rdd1.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T15:09:52.083178Z",
     "start_time": "2025-07-28T15:09:47.787151Z"
    }
   },
   "id": "c6b7fc3f4d05ec7d",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after repartitioning: 2\n",
      "Repartitioned RDD content: [7, 8, 9, 10, 1, 2, 3, 4, 5, 6]\n",
      "Number of partitions after coalesce: 1\n",
      "Repartitioned RDD content: [7, 8, 9, 10, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# 重新分区：repartition()会触发shuffle适合需要均匀数据的场景、coalesce()合并相邻分区，适合减少分区数的场景\n",
    "rdd2 = rdd1.repartition(2)\n",
    "print(\"Number of partitions after repartitioning:\", rdd2.getNumPartitions())\n",
    "print(\"Repartitioned RDD content:\", rdd2.collect())\n",
    "\n",
    "rdd3 = rdd2.coalesce(1)\n",
    "print(\"Number of partitions after coalesce:\", rdd3.getNumPartitions())\n",
    "print(\"Repartitioned RDD content:\", rdd3.collect())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T15:13:40.899762Z",
     "start_time": "2025-07-28T15:13:40.487005Z"
    }
   },
   "id": "df868f7b5119fb51",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted RDD content: [(27, 'This'), (27, 'eBook'), (27, 'is'), (27, 'for'), (27, 'the'), (27, 'use'), (27, 'of'), (27, 'anyone'), (27, 'anywhere'), (27, 'at'), (27, 'no'), (27, 'cost'), (27, 'and'), (27, 'with'), (18, 'Alice’s'), (18, 'Adventures'), (18, 'in'), (18, 'Wonderland'), (18, 'by'), (18, 'Lewis'), (18, 'Carroll'), (9, 'Project'), (9, 'Gutenberg’s')]\n"
     ]
    }
   ],
   "source": [
    "# Transformations-惰性操作，返回新的rdd,常用操作算子：flatMap(), map(), reduceByKey(), filter(), sortByKey() \n",
    "rdd = sc.textFile(\"./data/test.txt\")  # 从文本文件创建RDD\n",
    "#flatMap()：和map类似，通过先将函数应用于此 RDD 的所有元素，然后展平结果（去掉嵌套），返回一个新的 RDD。\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "#map()：将每个输入元素映射到一个输出元素，返回一个新的RDD。\n",
    "rdd1 = rdd2.map(lambda x: (x,1))\n",
    "# reduceByKey()：对具有相同键的值进行聚合，返回一个新的RDD。\n",
    "rdd3 = rdd1.reduceByKey(lambda x, y: x + y)\n",
    "# sortByKey()：根据键对元素进行排序，返回一个新的RDD。\n",
    "rdd4 = rdd3.map(lambda x: (x[1], x[0])).sortByKey(ascending=False)\n",
    "print(\"Sorted RDD content:\", rdd4.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T15:31:42.156872Z",
     "start_time": "2025-07-28T15:31:41.872084Z"
    }
   },
   "id": "d8208d8d8b869b1f",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of elements in RDD: 23\n",
      "First element in RDD: 27 This\n",
      "Max element in RDD: 27 with\n",
      "Reduced result: 522\n",
      "First 3 elements in RDD: [(27, 'This'), (27, 'eBook'), (27, 'is')]\n",
      "All elements in RDD: [(27, 'This'), (27, 'eBook'), (27, 'is'), (27, 'for'), (27, 'the'), (27, 'use'), (27, 'of'), (27, 'anyone'), (27, 'anywhere'), (27, 'at'), (27, 'no'), (27, 'cost'), (27, 'and'), (27, 'with'), (18, 'Alice’s'), (18, 'Adventures'), (18, 'in'), (18, 'Wonderland'), (18, 'by'), (18, 'Lewis'), (18, 'Carroll'), (9, 'Project'), (9, 'Gutenberg’s')]\n"
     ]
    }
   ],
   "source": [
    "# action-行动操作，触发计算并返回RDD的值\n",
    "# count()：返回RDD中元素的数量\n",
    "count = rdd4.count()\n",
    "print(\"Count of elements in RDD:\", count)\n",
    "# first()：返回RDD中的第一个元素\n",
    "first = rdd4.first()\n",
    "print(\"First element in RDD:\",first[0],first[1] )\n",
    "# max()：返回RDD中最大元素\n",
    "max = rdd4.max()\n",
    "print(\"Max element in RDD:\", max[0],max[1])\n",
    "# reduce()：对RDD中的所有元素进行聚合操作\n",
    "reduce_result = rdd4.reduce(lambda x, y: (x[0] + y[0], x[1] + \" \" + y[1]))\n",
    "print(\"Reduced result:\", reduce_result[0])\n",
    "# take(n)：返回RDD中的前n个元素\n",
    "take_result = rdd4.take(3)\n",
    "print(\"First 3 elements in RDD:\", take_result)\n",
    "# collect()：返回RDD中的所有元素\n",
    "collect_result = rdd4.collect()\n",
    "print(\"All elements in RDD:\", collect_result)\n",
    "# saveAsTextFile(path)：将RDD保存到指定路径\n",
    "rdd4.saveAsTextFile(\"./data/output.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-28T15:38:01.380311Z",
     "start_time": "2025-07-28T15:38:00.206661Z"
    }
   },
   "id": "30804ecbbc670891",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "352fe3e646e4198e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
